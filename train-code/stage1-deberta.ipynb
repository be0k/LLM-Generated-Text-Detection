{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00292bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = 'output'\n",
    "    checkpoint: str = \"kakaocorp/kanana-1.5-2.1b-instruct-2505\"  # 4-bit quantized gemma-2-9b-instruct\n",
    "    max_length: int = 4096\n",
    "    n_splits: int = 5\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_torch\"\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4  # global batch size is 16\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 3\n",
    "    freeze_layers: int = 0  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 1e-5\n",
    "    warmup_steps: int = 20\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: float = 16\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_bias: str = \"none\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    do_eval=True,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",  # eval_strategy -> evaluation_strategy (올바른 키)\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    save_steps=200,\n",
    "    optim=config.optim_type,\n",
    "    learning_rate=config.lr,\n",
    "    weight_decay=0.01,  # ✅ weight decay 추가\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.truncation_side='right'\n",
    "tokenizer.add_eos_token = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,                    # ✅ 8bit quantization 사용\n",
    "    llm_int8_threshold=6.0,               # 기본값 (optional, 안정성 설정)\n",
    "    llm_int8_has_fp16_weight=True,        # fp16 weight 유지 → mixed precision 가능\n",
    "    llm_int8_skip_modules=[\"score\"]       # quantization 제외할 모듈\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217bff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\"],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=[\"score\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    num_labels=1,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "#model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ds = pd.read_csv(\"train.csv\")\n",
    "ds = Dataset.from_pandas(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f08563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def tmp(self, text):\n",
    "        return re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        title = [\"<Title>: \" + t for t in batch[\"title\"]]\n",
    "        full = [\"\\n\\n<Full text>: \" + self.tmp(t) for t in batch[\"full_text\"]]\n",
    "        texts = [t + f for t, f in zip(title, full)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "        labels = [float(label) for label in batch[\"generated\"]]\n",
    "        \n",
    "        return {**tokenized, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds = ds.map(encode, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions  # shape: (batch_size, 1)\n",
    "    labels = eval_preds.label_ids   # shape: (batch_size,)\n",
    "\n",
    "    # Apply sigmoid to get probabilities\n",
    "    probs = torch.from_numpy(preds).float().sigmoid().numpy().squeeze()\n",
    "\n",
    "    # Convert probabilities to binary predictions (threshold = 0.5)\n",
    "    binary_preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    # Compute metrics\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)\n",
    "    auc = roc_auc_score(y_true=labels, y_score=probs)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=binary_preds)\n",
    "\n",
    "    return {\"auc\": auc, \"log_loss\": loss, \"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaeed00-cc62-424c-9a0c-38724b219c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_based_split_indices(dataset, max_len=512):\n",
    "    train_idx = []\n",
    "    eval_idx = []\n",
    "\n",
    "    for i, input_ids in enumerate(dataset['input_ids']):\n",
    "        if len(input_ids) < max_len:\n",
    "            train_idx.append(i)\n",
    "        else:\n",
    "            eval_idx.append(i)\n",
    "\n",
    "    return train_idx, eval_idx\n",
    "\n",
    "# 사용 예시\n",
    "train_idx, eval_idx = get_length_based_split_indices(ds, max_len=4096)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127b333-9350-4031-b0f6-a95e4f8b8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom loss computation. Receives `inputs` dictionary and returns the loss.\n",
    "        You can add custom logic here.\n",
    "        \"\"\"\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")  # 또는 outputs.logits\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # 원하는 loss function 사용 예시\n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1ae74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = CustomTrainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b012c-26df-49db-9680-b03c5ea15ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba0024-f17f-450d-8cfd-8e6746e8ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf6d7d7-02a3-4ef2-9838-eb72b9fe9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "from transformers import PreTrainedTokenizerBase, AutoTokenizer\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _split_into_chunks(self, text: str, prefix: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        텍스트를 문장 단위로 분리하고, max_length에 맞춰 여러 청크로 나눕니다.\n",
    "        각 청크는 title_prefix와 full_text_prefix를 포함하여 시작합니다.\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "\n",
    "        \n",
    "        # [CLS], [SEP] 토큰 등을 고려하여 실제 텍스트에 사용할 수 있는 최대 토큰 수를 계산\n",
    "        # 일반적으로 max_length - 2 (CLS, SEP) 이지만, 사용하는 모델의 토크나이저 특성에 따라 다를 수 있습니다.\n",
    "        # 여기서는 간단히 max_length - 2로 가정합니다.\n",
    "        effective_max_length = self.max_length - 2\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk_sentences = []\n",
    "        current_chunk_token_length = 0\n",
    "\n",
    "        # 첫 번째 청크의 접두사 길이 계산 (예: \"<Title>: \" + \"\\n\\n<Full text>: \")\n",
    "        # 이 길이는 각 청크의 시작 부분에 포함될 것이므로, effective_max_length에서 제외해야 합니다.\n",
    "        # 실제 청크에 들어갈 내용의 토큰 길이만 고려하기 위함입니다.\n",
    "        initial_prefix_tokens = self.tokenizer.encode(prefix, add_special_tokens=False)\n",
    "        prefix_token_length = len(initial_prefix_tokens)\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence_tokens = self.tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            \n",
    "            # 새 청크의 시작이라면 접두사 길이를 포함하여 계산\n",
    "            # 그렇지 않으면 문장 사이의 공백 토큰을 고려 (+1 또는 +a)\n",
    "            len_to_add = len(sentence_tokens)\n",
    "            if not current_chunk_sentences: # 새 청크의 첫 문장\n",
    "                len_to_add += prefix_token_length\n",
    "            else: # 기존 청크에 문장 추가\n",
    "                len_to_add += len(self.tokenizer.encode(\" \", add_special_tokens=False)) # 공백 토큰 길이\n",
    "\n",
    "            if current_chunk_token_length + len_to_add > effective_max_length:\n",
    "                # 현재까지 모아둔 문장들로 청크를 만듭니다.\n",
    "                chunk_body = \" \".join(current_chunk_sentences).strip()\n",
    "                # 새 청크에는 반드시 title_prefix와 full_text_prefix를 붙입니다.\n",
    "                chunks.append(prefix + chunk_body)\n",
    "                \n",
    "                # 새로운 청크를 현재 문장으로 시작\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_chunk_token_length = prefix_token_length + len(sentence_tokens)\n",
    "            else:\n",
    "                # 초과하지 않으면 현재 청크에 문장 추가\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                current_chunk_token_length += len_to_add\n",
    "\n",
    "        # 마지막 남은 문장들도 처리\n",
    "        if current_chunk_sentences:\n",
    "            chunk_body = \" \".join(current_chunk_sentences).strip()\n",
    "            chunks.append(prefix + chunk_body)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def __call__(self, batch: dict) -> Dict[str, List]:\n",
    "        # 각 원본 텍스트(제목 + 본문)에 대해 여러 청크를 생성합니다.\n",
    "        all_tokenized_input_ids = []\n",
    "        all_attention_mask = []\n",
    "        all_token_type_ids = [] # BERT 계열에서 사용될 수 있음\n",
    "        all_labels = []\n",
    "\n",
    "        # 배치 내 각 샘플을 개별적으로 처리\n",
    "        for i in range(len(batch[\"title\"])):\n",
    "\n",
    "            # _split_into_chunks를 호출하여 여러 청크를 얻습니다.\n",
    "            # 이 청크들은 이미 title_prefix와 full_text_prefix가 붙은 상태입니다.\n",
    "            processed_chunks = self._split_into_chunks(batch[\"full_text\"][i], \"<Title>: \" + batch[\"title\"][i] + \"\\n\\n<Full text>: \")\n",
    "\n",
    "            # 각 청크를 토큰화합니다.\n",
    "            # labels는 원본 샘플에 대해 하나만 존재한다고 가정하고, 첫 번째 청크에만 연결합니다.\n",
    "            # 만약 모든 청크에 동일한 라벨을 붙이고 싶다면, all_labels.extend([float(batch[\"generated\"][i])] * len(processed_chunks)) 로 변경\n",
    "            \n",
    "            # 훈련 시 첫 번째 청크만 사용하거나, 모든 청크를 개별 샘플로 취급할 수 있습니다.\n",
    "            # 여기서는 편의상 각 원본 샘플이 여러 청크를 가질 수 있도록 처리합니다.\n",
    "            \n",
    "            # 각 원본 데이터 포인트(batch[\"title\"][i] + batch[\"full_text\"][i])에 대해\n",
    "            # 생성된 모든 청크를 최종 결과에 추가합니다.\n",
    "            \n",
    "            # 주의: 만약 'labels'가 각 원본 데이터 포인트(combined_text)에 대한 것이고,\n",
    "            # 모델이 각 '청크'를 별도의 학습 샘플로 사용한다면,\n",
    "            # 'labels' 리스트도 그에 맞춰 복제되거나 할당되어야 합니다.\n",
    "            # 여기서는 하나의 원본 데이터에 대해 여러 청크가 생성되고,\n",
    "            # 해당 원본 데이터의 labels를 각 청크에 반복하여 할당하는 방식으로 구현합니다.\n",
    "            \n",
    "            for chunk in processed_chunks:\n",
    "                tokenized_chunk = self.tokenizer(\n",
    "                    chunk,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True, # 안전 장치\n",
    "                    return_tensors=\"pt\" # PyTorch 텐서로 반환 (선택 사항, 데이터셋 구성에 따라 다름)\n",
    "                )\n",
    "                all_tokenized_input_ids.append(tokenized_chunk['input_ids'].squeeze().tolist())\n",
    "                all_attention_mask.append(tokenized_chunk['attention_mask'].squeeze().tolist())\n",
    "                if 'token_type_ids' in tokenized_chunk: # BERT 계열에서만 존재\n",
    "                    all_token_type_ids.append(tokenized_chunk['token_type_ids'].squeeze().tolist())\n",
    "                \n",
    "                # 원본 라벨을 현재 청크에 할당\n",
    "                all_labels.append(float(batch[\"generated\"][i]))\n",
    "        \n",
    "        result = {\n",
    "            \"input_ids\": all_tokenized_input_ids,\n",
    "            \"attention_mask\": all_attention_mask,\n",
    "            \"labels\": all_labels\n",
    "        }\n",
    "        if all_token_type_ids:\n",
    "            result[\"token_type_ids\"] = all_token_type_ids\n",
    "            \n",
    "        return result\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"team-lucid/deberta-v3-base-korean\")\n",
    "bert_tokenizer.pad_token = tokenizer.eos_token\n",
    "bert_tokenizer.padding_side = 'right'\n",
    "bert_tokenizer.truncation_side='right'\n",
    "bert_tokenizer.add_eos_token = True\n",
    "custom_tokenizer = CustomTokenizer(tokenizer=bert_tokenizer, max_length=512) # 짧은 길이로 테스트\n",
    "\n",
    "ds = Dataset.from_csv(\"train.csv\")\n",
    "ds = ds.map(custom_tokenizer, batched=True, remove_columns=ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8acdc0-0126-495f-8a8f-c3d7eeeeb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        bert_tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        texts = [self.bert_tokenizer.decode(t, skip_special_tokens=True) for t in batch['input_ids']]\n",
    "\n",
    "        tokenized = self.tokenizer(texts)\n",
    "        \n",
    "        # 기존 input_ids도 보존\n",
    "        tokenized['original_input_ids'] = batch['input_ids']\n",
    "        tokenized['original_attention_mask'] = batch['attention_mask']  # 선택적으로 추가\n",
    "\n",
    "        # 기존 labels, token_type_ids도 보존하고 싶으면 아래처럼 추가\n",
    "        if 'labels' in batch:\n",
    "            tokenized['labels'] = batch['labels']\n",
    "        if 'token_type_ids' in batch:\n",
    "            tokenized['original_token_type_ids'] = batch['token_type_ids']\n",
    "\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf1ee5-cdec-45e8-b3a6-550a6cdcd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=1024, bert_tokenizer=bert_tokenizer)\n",
    "\n",
    "ds = ds.map(encode, batched=True, remove_columns=ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ecafd-76ae-4b94-b790-57bb4e791064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(ds, model, batch_size=8):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in tqdm(range(0, len(ds), batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, len(ds))\n",
    "        tmp = ds[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"]\n",
    "        attention_mask = tmp[\"attention_mask\"]\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(\"cuda:0\"))\n",
    "        proba = outputs.logits.cpu()\n",
    "        \n",
    "        preds.extend(proba[:, 0].tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4ae3d-7203-4767-a8d4-561ac8c66e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(ds, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8161d5-8b92-44ab-ab40-acc04ce9cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = ds.add_column(\"generated\", torch.sigmoid(torch.tensor(a)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4821355-7134-4809-b625-78387a5c5e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d367935-55c3-40de-b560-03a8f893f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = all_ds.remove_columns(['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859bce09-9e66-40e7-93c9-2af689995735",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds.save_to_disk(\"./stage1_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3663f79-b2cf-4d06-9fbd-d271676cc975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3847953-fda4-4546-a56e-a3b2f3f44c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
