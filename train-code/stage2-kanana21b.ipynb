{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da58d41-4f5b-4a3d-b974-8f63c3e4c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # CUDA 디바이스 0, 2만 사용하도록 설정\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00292bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = 'kanana21'\n",
    "    checkpoint: str = \"kakaocorp/kanana-1.5-2.1b-instruct-2505\"   # 4-bit quantized gemma-2-9b-instruct\n",
    "    max_length: int = 1024\n",
    "    n_splits: int = 20\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_torch\"\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4  # global batch size is 16\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 1\n",
    "    freeze_layers: int = 0  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 1e-4\n",
    "    warmup_steps: int = 20\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: float = 16\n",
    "    lora_dropout: float = 0.\n",
    "    lora_bias: str = \"none\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    do_eval=True,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    save_steps=200,\n",
    "    optim=config.optim_type,\n",
    "    weight_decay=1e-2,\n",
    "    fp16=True,\n",
    "    learning_rate=config.lr,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    dataloader_num_workers=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.checkpoint)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.truncation_side='right'\n",
    "tokenizer.add_eos_token = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4aed8-77a2-46aa-976d-8ea8a8aea6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,                    # ✅ 8bit quantization 사용\n",
    "    llm_int8_skip_modules=[\"score\"]       # quantization 제외할 모듈\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bcf44b-2ce0-40a4-b9c3-a02d490d2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\"],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=[\"score\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    num_labels=2,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "#model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729742a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"./stage1_kanana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23746d6b-3434-4149-93a5-8d176a1172f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(lambda x: {'labels': [x['labels'], x['generated']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04138ae-ab6d-427d-b34b-de6d380270cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 'generated' 컬럼 제거\n",
    "ds = ds.remove_columns(['generated'])\n",
    "\n",
    "# Step 2: 컬럼 이름 변경\n",
    "ds = ds.rename_columns({\n",
    "    \"original_input_ids\": \"input_ids\",\n",
    "    \"original_attention_mask\": \"attention_mask\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc39358-dd81-4717-a860-6a34babdea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions  # shape: (batch_size, 2)\n",
    "    labels = eval_preds.label_ids   # shape: (batch_size, 2)\n",
    "\n",
    "    probs = torch.from_numpy(preds).float().sigmoid().numpy()  # shape: (batch_size, 2)\n",
    "    binary_preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    results = {}\n",
    "    task_names = [\"labels\", \"generated\"]\n",
    "\n",
    "    for i, task in enumerate(task_names):\n",
    "        y_true = labels[:, i]\n",
    "        y_prob = probs[:, i]\n",
    "        y_pred = binary_preds[:, i]\n",
    "\n",
    "        results[f\"{task}_log_loss\"] = log_loss(y_true, y_prob)\n",
    "        results[f\"{task}_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "        results[f\"{task}_accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "        break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638cb0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def get_train_val_split_indices(dataset, n_splits):\n",
    "    total = len(dataset)\n",
    "    split_size = ceil(total / n_splits)\n",
    "    indices = list(range(total))\n",
    "\n",
    "    # Split into n_splits parts\n",
    "    splits = [indices[i*split_size:(i+1)*split_size] for i in range(n_splits)]\n",
    "\n",
    "    # Train: all but last split\n",
    "    train_indices = [i for split in splits[:-1] for i in split]\n",
    "    val_indices = splits[-1]\n",
    "\n",
    "    return train_indices, val_indices\n",
    "\n",
    "# 예시 사용:\n",
    "train_idx, eval_idx = get_train_val_split_indices(ds, config.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc093ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")  # shape: (batch_size, 2)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")  # shape: (batch_size, 2)\n",
    "\n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = CustomTrainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b651fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840df9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        title = [\"<Title>: \" + t for t in batch[\"title\"]]\n",
    "        para = [\"\\n\\n<Full text>: \" + t for t in batch[\"paragraph_text\"]]\n",
    "        texts = [t + p for t, p in zip(title, para)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "\n",
    "        return {**tokenized}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=1024)\n",
    "ds = ds.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(ds, model, batch_size=1):\n",
    "    preds = []\n",
    "    pseudo = []\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in tqdm(range(0, len(ds), batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, len(ds))\n",
    "        tmp = ds[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"]\n",
    "        attention_mask = tmp[\"attention_mask\"]\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(\"cuda:0\"))\n",
    "        proba = outputs.logits.cpu()\n",
    "        \n",
    "        preds.extend(proba[:, 0].tolist())\n",
    "        pseudo.extend(proba[:,1].tolist())\n",
    "    \n",
    "    return preds, pseudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = inference(ds, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71110ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub.head()\n",
    "\n",
    "sub['generated'] = a\n",
    "\n",
    "sub.to_csv('stage2_a_kanana21b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd39703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
