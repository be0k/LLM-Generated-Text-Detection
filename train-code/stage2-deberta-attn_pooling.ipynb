{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "# CUDA 디바이스 0, 2만 사용하도록 설정\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    #밑에 저장 이름도\n",
    "    output_dir: str = 'stage2_attpool_cls_2l'\n",
    "    checkpoint: str = \"team-lucid/deberta-v3-base-korean\"  # 4-bit quantized gemma-2-9b-instruct\n",
    "    max_length: int = 512\n",
    "    n_splits: int = 20\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_torch\"\n",
    "    per_device_train_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 1  # global batch size is 16\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 5\n",
    "    freeze_layers: int = 0  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 1e-5\n",
    "    warmup_steps: int = 20\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: float = 16\n",
    "    lora_dropout: float = 0.\n",
    "    lora_bias: str = \"none\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    do_eval=True,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    # save_steps=200,\n",
    "    optim=config.optim_type,\n",
    "    weight_decay=1e-2,\n",
    "    fp16=True,\n",
    "    learning_rate=config.lr,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    dataloader_num_workers=64\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.truncation_side='right'\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "\n",
    "###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModel, PreTrainedModel\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        scores = self.attention(last_hidden_state).squeeze(-1)\n",
    "        scores = scores.masked_fill(attention_mask == 0, float('-inf')) #B x L\n",
    "        attn_weights = F.softmax(scores, dim=1).unsqueeze(-1) #B x L x 1\n",
    "        \n",
    "        weighted_sum = torch.sum(last_hidden_state * attn_weights, dim=1)\n",
    "        return weighted_sum\n",
    "\n",
    "\n",
    "class AiModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    Hugging Face Trainer-compatible AI Model with Attention Pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        print(\"Initializing the AI Model...\")\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(config._name_or_path, config=config)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "\n",
    "        self.pool = AttentionPooling(hidden_size)\n",
    "\n",
    "        # self.projection_head = nn.Linear(hidden_size, 1)\n",
    "        # self.projection_head = nn.Linear(hidden_size, 2) #1 -> 2\n",
    "        # self.projection_head = nn.Linear(hidden_size*2, 2) #1 -> 2 (use cls concat)\n",
    "\n",
    "        self.projection_head = nn.Sequential(nn.Linear(hidden_size*2, hidden_size, bias = True),\n",
    "                                        nn.Linear(hidden_size, 2, bias = True),\n",
    "                                        nn.Dropout(p=0.1, inplace=False)) #1 (use cls concat)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "\n",
    "        ###BASE\n",
    "        # last_hidden_state = outputs.last_hidden_state #B x 508 x H(768)\n",
    "        # pooled_output = self.pool(last_hidden_state, attention_mask) #B x H(768)\n",
    "        # logits = self.projection_head(pooled_output).squeeze(-1)  # shape: (batch_size)\n",
    "        # logits = self.projection_head(pooled_output).squeeze(-1)  # shape: (batch_size)\n",
    "\n",
    "\n",
    "        # ## USE CLS concat\n",
    "        cls_hidden = outputs.last_hidden_state[:, 0, :] #B x 1 x H\n",
    "        rest_hidden_state = outputs.last_hidden_state[:, 1:, :]  #B x (508-1) x H(768)\n",
    "        pooled_output = self.pool(rest_hidden_state, attention_mask[:, 1:]) #B x H(768)\n",
    "\n",
    "        my_hidden = torch.concat([cls_hidden, pooled_output], dim = 1) #B x H*2\n",
    "        logits = self.projection_head(my_hidden)  # shape: B x 2\n",
    "        # ##-----------\n",
    "\n",
    "        ##-----------\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels.float())\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "###-------------------------------------\n",
    "from transformers import AutoConfig, AutoModel, PreTrainedModel\n",
    "\n",
    "m_config = AutoConfig.from_pretrained(config.checkpoint)\n",
    "model = AiModel(m_config)\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"./stage1_llm\")\n",
    "if False:\n",
    "    ds = ds.select(range(200))\n",
    "    # 첫 번째 샘플의 labels만 1로 수정\n",
    "ds = ds.map(lambda x: {'labels': [x['labels'], x['generated']]})\n",
    "\n",
    "# Step 1: 'generated' 컬럼 제거\n",
    "ds = ds.remove_columns(['generated'])\n",
    "\n",
    "# Step 2: 컬럼 이름 변경\n",
    "ds = ds.rename_columns({\n",
    "    \"original_input_ids\": \"input_ids\",\n",
    "    \"original_attention_mask\": \"attention_mask\",\n",
    "    \"original_token_type_ids\": \"token_type_ids\",\n",
    "})\n",
    "\n",
    "\n",
    "#\n",
    "#---------\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions  # shape: (batch_size, 2)\n",
    "    labels = eval_preds.label_ids   # shape: (batch_size, 2)\n",
    "\n",
    "    probs = torch.from_numpy(preds).float().sigmoid().numpy()  # shape: (batch_size, 2)\n",
    "    binary_preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    results = {}\n",
    "    task_names = [\"labels\", \"generated\"]\n",
    "\n",
    "    for i, task in enumerate(task_names):\n",
    "        y_true = labels[:, i]\n",
    "        y_prob = probs[:, i]\n",
    "        y_pred = binary_preds[:, i]\n",
    "\n",
    "        results[f\"{task}_log_loss\"] = log_loss(y_true, y_prob)\n",
    "        results[f\"{task}_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "        results[f\"{task}_accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "        break\n",
    "    return results\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "def get_train_val_split_indices(dataset, n_splits):\n",
    "    total = len(dataset)\n",
    "    split_size = ceil(total / n_splits)\n",
    "    indices = list(range(total))\n",
    "\n",
    "    # Split into n_splits parts\n",
    "    splits = [indices[i*split_size:(i+1)*split_size] for i in range(n_splits)]\n",
    "\n",
    "    # Train: all but last split\n",
    "    train_indices = [i for split in splits[:-1] for i in split]\n",
    "    val_indices = splits[-1]\n",
    "\n",
    "    return train_indices, val_indices\n",
    "\n",
    "# 예시 사용:\n",
    "train_idx, eval_idx = get_train_val_split_indices(ds, config.n_splits)\n",
    "\n",
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")  # shape: (batch_size, 2)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")  # shape: (batch_size, 2)\n",
    "\n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "ds = Dataset.from_csv(\"./test.csv\")\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        title = [\"<Title>: \" + t for t in batch[\"title\"]]\n",
    "        para = [\"\\n\\n<Full text>: \" + t for t in batch[\"paragraph_text\"]]\n",
    "        texts = [t + p for t, p in zip(title, para)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "\n",
    "        return {**tokenized}\n",
    "    \n",
    "encode = CustomTokenizer(tokenizer, max_length=512)\n",
    "ds = ds.map(encode, batched=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(ds, model, batch_size=1):\n",
    "    preds = []\n",
    "    pseudo = []\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in tqdm(range(0, len(ds), batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, len(ds))\n",
    "        tmp = ds[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"]\n",
    "        attention_mask = tmp[\"attention_mask\"]\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(\"cuda:0\"))\n",
    "        proba = outputs.get(\"logits\").cpu()\n",
    "        \n",
    "        preds.extend(proba[:, 0].tolist())\n",
    "        pseudo.extend(proba[:,1].tolist())\n",
    "    \n",
    "    return preds, pseudo\n",
    "\n",
    "a, b = inference(ds, model)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sub = pd.read_csv('./sample_submission.csv')\n",
    "sub.head()\n",
    "\n",
    "sub['generated'] = b\n",
    "\n",
    "sub.to_csv('stage2_attpool_b.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
