{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CUDA 디바이스 0, 2만 사용하도록 설정\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00292bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = 'stage2_deberta_1:2'\n",
    "    checkpoint: str = \"team-lucid/deberta-v3-base-korean\" # 4-bit quantized gemma-2-9b-instruct\n",
    "    max_length: int = 512\n",
    "    n_splits: int = 20\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_torch\"\n",
    "    per_device_train_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 1  # global batch size is 16\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 5\n",
    "    freeze_layers: int = 0  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 1e-5\n",
    "    warmup_steps: int = 20\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: float = 16\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_bias: str = \"none\"\n",
    "    real_label_weight: float = 0.33\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    do_eval=True,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    # eval_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    save_steps=200,\n",
    "    optim=config.optim_type,\n",
    "    weight_decay=1e-2,\n",
    "    fp16=True,\n",
    "    learning_rate=config.lr,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    dataloader_num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.truncation_side='right'\n",
    "tokenizer.add_eos_token = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     config.checkpoint ,\n",
    "#     num_labels=2,\n",
    "#     device_map=\"cuda:0\",\n",
    "# )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    num_labels=1,  \n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729742a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"./stage1_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23746d6b-3434-4149-93a5-8d176a1172f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(lambda x: {'labels': [x['labels'], x['generated']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04138ae-ab6d-427d-b34b-de6d380270cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 'generated' 컬럼 제거\n",
    "ds = ds.remove_columns(['generated'])\n",
    "\n",
    "# Step 2: 컬럼 이름 변경\n",
    "ds = ds.rename_columns({\n",
    "    \"original_input_ids\": \"input_ids\",\n",
    "    \"original_attention_mask\": \"attention_mask\",\n",
    "    \"original_token_type_ids\": \"token_type_ids\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00960523",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "# def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "#     preds = eval_preds.predictions  # shape: (batch_size, 1)\n",
    "#     labels = eval_preds.label_ids   # shape: (batch_size,)\n",
    "\n",
    "#     # Apply sigmoid to get probabilities\n",
    "#     probs = torch.from_numpy(preds).float().sigmoid().numpy().squeeze()\n",
    "\n",
    "#     # Convert probabilities to binary predictions (threshold = 0.5)\n",
    "#     binary_preds = (probs >= 0.5).astype(int)\n",
    "#     binary_labels = (labels >= 0.5).astype(int)\n",
    "#     # Compute metrics\n",
    "#     loss = log_loss(y_true=binary_labels, y_pred=probs)\n",
    "#     auc = roc_auc_score(y_true=binary_labels, y_score=probs)\n",
    "#     acc = accuracy_score(y_true=binary_labels, y_pred=binary_preds)\n",
    "\n",
    "#     return {\"auc\": auc, \"log_loss\": loss, \"accuracy\": acc}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions  # shape: (batch_size,) or (batch_size, 1)\n",
    "    labels = eval_preds.label_ids   # shape: (batch_size, 2)\n",
    "\n",
    "    probs = torch.from_numpy(preds).float().sigmoid().numpy().squeeze()\n",
    "    real_labels = labels[:, 0]  # real label만 사용\n",
    "\n",
    "    # log_loss expects y_pred to be probs between 0 and 1 (1D)\n",
    "    loss = log_loss(y_true=real_labels, y_pred=probs)\n",
    "    auc = roc_auc_score(y_true=real_labels, y_score=probs)\n",
    "    binary_preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true=real_labels, y_pred=binary_preds)\n",
    "\n",
    "    return {\"auc\": auc, \"log_loss\": loss, \"accuracy\": acc}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638cb0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def get_train_val_split_indices(dataset, n_splits):\n",
    "    total = len(dataset)\n",
    "    split_size = ceil(total / n_splits)\n",
    "    indices = list(range(total))\n",
    "\n",
    "    # Split into n_splits parts\n",
    "    splits = [indices[i*split_size:(i+1)*split_size] for i in range(n_splits)]\n",
    "\n",
    "    # Train: all but last split\n",
    "    train_indices = [i for split in splits[:-1] for i in split]\n",
    "    val_indices = splits[-1]\n",
    "\n",
    "    return train_indices, val_indices\n",
    "\n",
    "# 예시 사용:\n",
    "train_idx, eval_idx = get_train_val_split_indices(ds, config.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65325ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "@dataclass\n",
    "class CustomDataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: bool = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float)\n",
    "\n",
    "        for f in features:\n",
    "            f.pop(\"labels\")\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch[\"labels\"] = labels  # (batch_size, 2)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc093ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "#         labels = inputs.get(\"labels\")  # shape: (batch_size, 2)\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.get(\"logits\")  # shape: (batch_size, 2)\n",
    "\n",
    "#         real_logit = logits[:, 0]\n",
    "#         pseudo_logit = logits[:, 1]\n",
    "#         real_label = labels[:, 0]\n",
    "#         pseudo_label = labels[:, 1]\n",
    "\n",
    "#         loss_fct = nn.BCEWithLogitsLoss()\n",
    "#         loss_real = loss_fct(real_logit, real_label.float())\n",
    "#         loss_pseudo = loss_fct(pseudo_logit, pseudo_label.float())\n",
    "\n",
    "#         REAL_WEIGHT = config.real_label_weight\n",
    "#         PSEUDO_WEIGHT = 1 - REAL_WEIGHT\n",
    "#         loss = REAL_WEIGHT * loss_real + PSEUDO_WEIGHT * loss_pseudo\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")  # (batch_size, 2)\n",
    "        labels = labels.to(model.device).float()\n",
    "\n",
    "        real_label = labels[:, 0]\n",
    "        pseudo_label = labels[:, 1]\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[\"logits\"].squeeze(-1)  # (batch_size,)\n",
    "\n",
    "\n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "        loss_real = loss_fct(logits, real_label)\n",
    "        loss_pseudo = loss_fct(logits, pseudo_label)\n",
    "\n",
    "        REAL_WEIGHT = config.real_label_weight\n",
    "        PSEUDO_WEIGHT = 1 - REAL_WEIGHT\n",
    "        loss = REAL_WEIGHT * loss_real + PSEUDO_WEIGHT * loss_pseudo\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"총 train samples: {len(train_idx)}\")\n",
    "print(f\"총 val samples: {len(eval_idx)}\")\n",
    "\n",
    "steps_per_epoch = (len(train_idx) + config.per_device_train_batch_size - 1) // config.per_device_train_batch_size\n",
    "total_steps = steps_per_epoch * config.n_epochs\n",
    "print(f\"total steps: {total_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = CustomTrainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b651fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840df9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        title = [\"<Title>: \" + t for t in batch[\"title\"]]\n",
    "        para = [\"\\n\\n<Full text>: \" + t for t in batch[\"paragraph_text\"]]\n",
    "        texts = [t + p for t, p in zip(title, para)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "\n",
    "        return {**tokenized}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=512)\n",
    "ds = ds.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(ds, model, batch_size=1):\n",
    "    preds = []\n",
    "    pseudo = []\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in tqdm(range(0, len(ds), batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, len(ds))\n",
    "        tmp = ds[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"]\n",
    "        attention_mask = tmp[\"attention_mask\"]\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(\"cuda:0\"))\n",
    "        # proba = outputs.logits.cpu()\n",
    "        logits = outputs.logits.squeeze(-1).cpu()\n",
    "        proba = torch.sigmoid(logits)\n",
    "\n",
    "        \n",
    "        # preds.extend(proba[:, 0].tolist())\n",
    "        # pseudo.extend(proba[:,1].tolist())\n",
    "        preds.extend(proba.tolist())  # logit 하나\n",
    "\n",
    "    \n",
    "    return preds, pseudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, _ = inference(ds, model)  \n",
    "\n",
    "import pandas as pd\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "sub['generated'] = probs  \n",
    "\n",
    "sub.to_csv('stage2_0.33:0.67_total_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfc332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
