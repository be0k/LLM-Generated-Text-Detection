{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2cfc332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the AI Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2Model were not initialized from the model checkpoint at RowKick/deberta-v3-base-korean-attention-pooling-stage2 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.LayerNorm.bias', 'encoder.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key_proj.bias', 'encoder.layer.0.attention.self.key_proj.weight', 'encoder.layer.0.attention.self.query_proj.bias', 'encoder.layer.0.attention.self.query_proj.weight', 'encoder.layer.0.attention.self.value_proj.bias', 'encoder.layer.0.attention.self.value_proj.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key_proj.bias', 'encoder.layer.1.attention.self.key_proj.weight', 'encoder.layer.1.attention.self.query_proj.bias', 'encoder.layer.1.attention.self.query_proj.weight', 'encoder.layer.1.attention.self.value_proj.bias', 'encoder.layer.1.attention.self.value_proj.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key_proj.bias', 'encoder.layer.10.attention.self.key_proj.weight', 'encoder.layer.10.attention.self.query_proj.bias', 'encoder.layer.10.attention.self.query_proj.weight', 'encoder.layer.10.attention.self.value_proj.bias', 'encoder.layer.10.attention.self.value_proj.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key_proj.bias', 'encoder.layer.11.attention.self.key_proj.weight', 'encoder.layer.11.attention.self.query_proj.bias', 'encoder.layer.11.attention.self.query_proj.weight', 'encoder.layer.11.attention.self.value_proj.bias', 'encoder.layer.11.attention.self.value_proj.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key_proj.bias', 'encoder.layer.2.attention.self.key_proj.weight', 'encoder.layer.2.attention.self.query_proj.bias', 'encoder.layer.2.attention.self.query_proj.weight', 'encoder.layer.2.attention.self.value_proj.bias', 'encoder.layer.2.attention.self.value_proj.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key_proj.bias', 'encoder.layer.3.attention.self.key_proj.weight', 'encoder.layer.3.attention.self.query_proj.bias', 'encoder.layer.3.attention.self.query_proj.weight', 'encoder.layer.3.attention.self.value_proj.bias', 'encoder.layer.3.attention.self.value_proj.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key_proj.bias', 'encoder.layer.4.attention.self.key_proj.weight', 'encoder.layer.4.attention.self.query_proj.bias', 'encoder.layer.4.attention.self.query_proj.weight', 'encoder.layer.4.attention.self.value_proj.bias', 'encoder.layer.4.attention.self.value_proj.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key_proj.bias', 'encoder.layer.5.attention.self.key_proj.weight', 'encoder.layer.5.attention.self.query_proj.bias', 'encoder.layer.5.attention.self.query_proj.weight', 'encoder.layer.5.attention.self.value_proj.bias', 'encoder.layer.5.attention.self.value_proj.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key_proj.bias', 'encoder.layer.6.attention.self.key_proj.weight', 'encoder.layer.6.attention.self.query_proj.bias', 'encoder.layer.6.attention.self.query_proj.weight', 'encoder.layer.6.attention.self.value_proj.bias', 'encoder.layer.6.attention.self.value_proj.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key_proj.bias', 'encoder.layer.7.attention.self.key_proj.weight', 'encoder.layer.7.attention.self.query_proj.bias', 'encoder.layer.7.attention.self.query_proj.weight', 'encoder.layer.7.attention.self.value_proj.bias', 'encoder.layer.7.attention.self.value_proj.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key_proj.bias', 'encoder.layer.8.attention.self.key_proj.weight', 'encoder.layer.8.attention.self.query_proj.bias', 'encoder.layer.8.attention.self.query_proj.weight', 'encoder.layer.8.attention.self.value_proj.bias', 'encoder.layer.8.attention.self.value_proj.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key_proj.bias', 'encoder.layer.9.attention.self.key_proj.weight', 'encoder.layer.9.attention.self.query_proj.bias', 'encoder.layer.9.attention.self.query_proj.weight', 'encoder.layer.9.attention.self.value_proj.bias', 'encoder.layer.9.attention.self.value_proj.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.rel_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1962/1962 [00:16<00:00, 121.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "# CUDA 디바이스 0, 2만 사용하도록 설정\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "# 예시: \"your-username/your-model-name\"\n",
    "model_name = \"RowKick/deberta-v3-base-korean-attention-pooling-stage2\"\n",
    "\n",
    "# tokenizer 불러오기 (필요한 경우)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# config 불러오기\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.truncation_side='right'\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "\n",
    "###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModel, PreTrainedModel\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        scores = self.attention(last_hidden_state).squeeze(-1)\n",
    "        scores = scores.masked_fill(attention_mask == 0, float('-inf')) #B x L\n",
    "        attn_weights = F.softmax(scores, dim=1).unsqueeze(-1) #B x L x 1\n",
    "        \n",
    "        weighted_sum = torch.sum(last_hidden_state * attn_weights, dim=1)\n",
    "        return weighted_sum\n",
    "\n",
    "\n",
    "class AiModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    Hugging Face Trainer-compatible AI Model with Attention Pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        print(\"Initializing the AI Model...\")\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(config._name_or_path, config=config)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "\n",
    "        self.pool = AttentionPooling(hidden_size)\n",
    "\n",
    "        # self.projection_head = nn.Linear(hidden_size, 1)\n",
    "        # self.projection_head = nn.Linear(hidden_size, 2) #1 -> 2\n",
    "        # self.projection_head = nn.Linear(hidden_size*2, 2) #1 -> 2 (use cls concat)\n",
    "\n",
    "        self.projection_head = nn.Sequential(nn.Linear(hidden_size*2, hidden_size, bias = True),\n",
    "                                        nn.Linear(hidden_size, 2, bias = True),\n",
    "                                        nn.Dropout(p=0.1, inplace=False)) #1 (use cls concat)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "\n",
    "        ###BASE\n",
    "        # last_hidden_state = outputs.last_hidden_state #B x 508 x H(768)\n",
    "        # pooled_output = self.pool(last_hidden_state, attention_mask) #B x H(768)\n",
    "        # logits = self.projection_head(pooled_output).squeeze(-1)  # shape: (batch_size)\n",
    "        # logits = self.projection_head(pooled_output).squeeze(-1)  # shape: (batch_size)\n",
    "\n",
    "\n",
    "        # ## USE CLS concat\n",
    "        cls_hidden = outputs.last_hidden_state[:, 0, :] #B x 1 x H\n",
    "        rest_hidden_state = outputs.last_hidden_state[:, 1:, :]  #B x (508-1) x H(768)\n",
    "        pooled_output = self.pool(rest_hidden_state, attention_mask[:, 1:]) #B x H(768)\n",
    "\n",
    "        my_hidden = torch.concat([cls_hidden, pooled_output], dim = 1) #B x H*2\n",
    "        logits = self.projection_head(my_hidden)  # shape: B x 2\n",
    "        # ##-----------\n",
    "\n",
    "        ##-----------\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels.float())\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "###-------------------------------------\n",
    "from transformers import AutoConfig, AutoModel, PreTrainedModel\n",
    "\n",
    "model = AiModel.from_pretrained(model_name, config=config)\n",
    "model.to('cuda:0')\n",
    "\n",
    "model.config.use_cache = False\n",
    " \n",
    "\n",
    "ds = Dataset.from_csv(\"./test.csv\")\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        title = [\"<Title>: \" + t for t in batch[\"title\"]]\n",
    "        para = [\"\\n\\n<Full text>: \" + t for t in batch[\"paragraph_text\"]]\n",
    "        texts = [t + p for t, p in zip(title, para)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "\n",
    "        return {**tokenized}\n",
    "    \n",
    "encode = CustomTokenizer(tokenizer, max_length=512)\n",
    "ds = ds.map(encode, batched=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(ds, model, batch_size=1):\n",
    "    preds = []\n",
    "    pseudo = []\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in tqdm(range(0, len(ds), batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, len(ds))\n",
    "        tmp = ds[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"]\n",
    "        attention_mask = tmp[\"attention_mask\"]\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(\"cuda:0\"))\n",
    "        proba = outputs.get(\"logits\").cpu()\n",
    "        \n",
    "        preds.extend(proba[:, 0].tolist())\n",
    "        pseudo.extend(proba[:,1].tolist())\n",
    "    \n",
    "    return preds, pseudo\n",
    "\n",
    "a, b = inference(ds, model)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sub = pd.read_csv('./sample_submission.csv')\n",
    "sub.head()\n",
    "\n",
    "sub['generated'] = b\n",
    "\n",
    "sub.to_csv('stage2_attpool_b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e7412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
